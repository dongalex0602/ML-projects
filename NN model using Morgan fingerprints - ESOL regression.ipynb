{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d839bbb",
   "metadata": {},
   "source": [
    "# Task.\n",
    "Using the ESOL dataset with 8/1/1 for train/valid/test splits and using RMSE as the performance measure: train a neural network model on the ESOL dataset by using Morgan fingerprints as input features and RMSE as the performance measure as well as the cost function. Report performance measure for all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b451abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4337af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09d32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c3a53",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f879c475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compound ID</th>\n",
       "      <th>ESOL predicted log solubility in mols per litre</th>\n",
       "      <th>Minimum Degree</th>\n",
       "      <th>Molecular Weight</th>\n",
       "      <th>Number of H-Bond Donors</th>\n",
       "      <th>Number of Rings</th>\n",
       "      <th>Number of Rotatable Bonds</th>\n",
       "      <th>Polar Surface Area</th>\n",
       "      <th>measured log solubility in mols per litre</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amigdalin</td>\n",
       "      <td>-0.974</td>\n",
       "      <td>1</td>\n",
       "      <td>457.432</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>202.32</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fenfuram</td>\n",
       "      <td>-2.885</td>\n",
       "      <td>1</td>\n",
       "      <td>201.225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42.24</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>Cc1occc1C(=O)Nc2ccccc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>citral</td>\n",
       "      <td>-2.579</td>\n",
       "      <td>1</td>\n",
       "      <td>152.237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17.07</td>\n",
       "      <td>-2.06</td>\n",
       "      <td>CC(C)=CCCC(C)=CC(=O)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Picene</td>\n",
       "      <td>-6.618</td>\n",
       "      <td>2</td>\n",
       "      <td>278.354</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7.87</td>\n",
       "      <td>c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thiophene</td>\n",
       "      <td>-2.232</td>\n",
       "      <td>2</td>\n",
       "      <td>84.143</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>c1ccsc1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Compound ID  ESOL predicted log solubility in mols per litre  \\\n",
       "0   Amigdalin                                           -0.974   \n",
       "1    Fenfuram                                           -2.885   \n",
       "2      citral                                           -2.579   \n",
       "3      Picene                                           -6.618   \n",
       "4   Thiophene                                           -2.232   \n",
       "\n",
       "   Minimum Degree  Molecular Weight  Number of H-Bond Donors  Number of Rings  \\\n",
       "0               1           457.432                        7                3   \n",
       "1               1           201.225                        1                2   \n",
       "2               1           152.237                        0                0   \n",
       "3               2           278.354                        0                5   \n",
       "4               2            84.143                        0                1   \n",
       "\n",
       "   Number of Rotatable Bonds  Polar Surface Area  \\\n",
       "0                          7              202.32   \n",
       "1                          2               42.24   \n",
       "2                          4               17.07   \n",
       "3                          0                0.00   \n",
       "4                          0                0.00   \n",
       "\n",
       "   measured log solubility in mols per litre  \\\n",
       "0                                      -0.77   \n",
       "1                                      -3.30   \n",
       "2                                      -2.06   \n",
       "3                                      -7.87   \n",
       "4                                      -1.33   \n",
       "\n",
       "                                              smiles  \n",
       "0  OCC3OC(OCC2OC(OC(C#N)c1ccccc1)C(O)C(O)C2O)C(O)...  \n",
       "1                             Cc1occc1C(=O)Nc2ccccc2  \n",
       "2                               CC(C)=CCCC(C)=CC(=O)  \n",
       "3                 c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43  \n",
       "4                                            c1ccsc1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esol = pd.read_csv(\"delaney_processed.csv\")\n",
    "esol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0d97d",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a69445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate canon SMILES\n",
    "def gen_canon_smiles(smiles_list):\n",
    "    \n",
    "    invalid_ids = []\n",
    "    canon_smiles = []\n",
    "\n",
    "    for i in range(len(smiles_list)):   \n",
    "        mol = Chem.MolFromSmiles(smiles_list[i])\n",
    "        \n",
    "        # do not append NoneType if invalid\n",
    "        if mol is None: \n",
    "            invalid_ids.append(i)\n",
    "            continue\n",
    "\n",
    "        canon_smiles.append(Chem.MolToSmiles(mol))\n",
    "\n",
    "    return canon_smiles, invalid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101120b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate morgan fingerprints from SMILES\n",
    "def calc_morgan_fpts(smiles_list):\n",
    "    morgan_fingerprints = []\n",
    "    \n",
    "    for i in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(i)\n",
    "        \n",
    "        # do not try to calculate if invalid\n",
    "        if mol is None: continue\n",
    "            \n",
    "        fpts = AllChem.GetMorganFingerprintAsBitVect(mol,2,2048)\n",
    "        mfpts = np.array(fpts)\n",
    "        morgan_fingerprints.append(mfpts) \n",
    "        \n",
    "    return np.array(morgan_fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0f0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate canon smiles\n",
    "canon_smiles, invalid_ids = gen_canon_smiles(esol.smiles)\n",
    "\n",
    "# drop rows with invalid SMILES\n",
    "esol = esol.drop(invalid_ids)\n",
    "\n",
    "# replace SMILES with canon SMILES\n",
    "esol.smiles = canon_smiles\n",
    "\n",
    "# drop duplicates to prevent train/valid/test contamination\n",
    "esol.drop_duplicates(subset=['smiles'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da094702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fingerprints and create TensorDataset\n",
    "X = torch.from_numpy(calc_morgan_fpts(esol.smiles)).float()\n",
    "y = torch.from_numpy(esol[\"measured log solubility in mols per litre\"].values).float()\n",
    "esol_ds = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f47752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training, validation, and test sets\n",
    "train_ds, valid_ds, test_ds = random_split(esol_ds, [0.80, 0.10, 0.10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca87e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e300c",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387848e",
   "metadata": {},
   "source": [
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2bbfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [32, 16]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e28614",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e7f1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 7.7337, Valid Loss: 2.3464\n",
      "Epoch 040, Train Loss: 4.6522, Valid Loss: 2.7143\n",
      "Epoch 060, Train Loss: 4.2944, Valid Loss: 2.6510\n",
      "Epoch 080, Train Loss: 3.8723, Valid Loss: 2.6809\n",
      "Epoch 100, Train Loss: 3.8019, Valid Loss: 2.6742\n",
      "Epoch 120, Train Loss: 3.7750, Valid Loss: 2.6409\n",
      "Epoch 140, Train Loss: 3.5653, Valid Loss: 2.6502\n",
      "Epoch 160, Train Loss: 3.6969, Valid Loss: 2.5938\n",
      "Epoch 180, Train Loss: 3.5051, Valid Loss: 2.6314\n",
      "Epoch 200, Train Loss: 3.6971, Valid Loss: 2.5945\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f66fe",
   "metadata": {},
   "source": [
    "#### Increase hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0c4f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ac7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b398e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 4.6639, Valid Loss: 2.2584\n",
      "Epoch 040, Train Loss: 4.0665, Valid Loss: 2.1762\n",
      "Epoch 060, Train Loss: 3.9779, Valid Loss: 2.0550\n",
      "Epoch 080, Train Loss: 3.7950, Valid Loss: 2.1702\n",
      "Epoch 100, Train Loss: 3.9314, Valid Loss: 2.1409\n",
      "Epoch 120, Train Loss: 3.7042, Valid Loss: 2.1247\n",
      "Epoch 140, Train Loss: 3.5746, Valid Loss: 2.1323\n",
      "Epoch 160, Train Loss: 3.6995, Valid Loss: 2.1275\n",
      "Epoch 180, Train Loss: 3.7217, Valid Loss: 2.1294\n",
      "Epoch 200, Train Loss: 3.6591, Valid Loss: 2.1263\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a1c80",
   "metadata": {},
   "source": [
    "**Increase hidden units again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "172d3e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a128ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58edb316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 4.5893, Valid Loss: 2.2734\n",
      "Epoch 040, Train Loss: 4.3788, Valid Loss: 2.1372\n",
      "Epoch 060, Train Loss: 4.0292, Valid Loss: 1.9974\n",
      "Epoch 080, Train Loss: 4.0408, Valid Loss: 2.1087\n",
      "Epoch 100, Train Loss: 3.9150, Valid Loss: 2.0536\n",
      "Epoch 120, Train Loss: 3.7056, Valid Loss: 2.0718\n",
      "Epoch 140, Train Loss: 3.7043, Valid Loss: 2.0739\n",
      "Epoch 160, Train Loss: 3.7188, Valid Loss: 2.0721\n",
      "Epoch 180, Train Loss: 3.6855, Valid Loss: 2.0499\n",
      "Epoch 200, Train Loss: 4.1616, Valid Loss: 2.0475\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05568fe0",
   "metadata": {},
   "source": [
    "```hidden_units=[512, 256]``` seems to give the best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251fb9e0",
   "metadata": {},
   "source": [
    "#### Add a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5661e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f058bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b80d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.5669, Valid Loss: 2.3301\n",
      "Epoch 040, Train Loss: 4.4709, Valid Loss: 2.1514\n",
      "Epoch 060, Train Loss: 4.0999, Valid Loss: 1.9826\n",
      "Epoch 080, Train Loss: 3.9182, Valid Loss: 2.0937\n",
      "Epoch 100, Train Loss: 3.8080, Valid Loss: 2.0679\n",
      "Epoch 120, Train Loss: 3.5776, Valid Loss: 2.0766\n",
      "Epoch 140, Train Loss: 3.6676, Valid Loss: 2.1193\n",
      "Epoch 160, Train Loss: 3.6619, Valid Loss: 2.0797\n",
      "Epoch 180, Train Loss: 3.7371, Valid Loss: 2.0759\n",
      "Epoch 200, Train Loss: 4.3581, Valid Loss: 2.0896\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1654cce",
   "metadata": {},
   "source": [
    "**Try adding a hidden layer with higher dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65783f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 512, 256]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b67bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bd86c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 4.8762, Valid Loss: 2.2480\n",
      "Epoch 040, Train Loss: 4.5860, Valid Loss: 2.0920\n",
      "Epoch 060, Train Loss: 4.2419, Valid Loss: 1.9641\n",
      "Epoch 080, Train Loss: 3.9658, Valid Loss: 2.0902\n",
      "Epoch 100, Train Loss: 3.9169, Valid Loss: 2.0401\n",
      "Epoch 120, Train Loss: 3.9030, Valid Loss: 2.0443\n",
      "Epoch 140, Train Loss: 3.6570, Valid Loss: 2.0290\n",
      "Epoch 160, Train Loss: 3.6731, Valid Loss: 2.0627\n",
      "Epoch 180, Train Loss: 4.1093, Valid Loss: 2.0600\n",
      "Epoch 200, Train Loss: 4.2778, Valid Loss: 2.0707\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973aa78",
   "metadata": {},
   "source": [
    "Adding a hidden layer with ```hidden_units=[512, 256, 64]``` gives the best performance on the validation set. Here it is interesting to note that increase the size of the hidden units did not improve the performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead13a0c",
   "metadata": {},
   "source": [
    "#### Try various activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c2f12",
   "metadata": {},
   "source": [
    "* **ReLU (current)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91377944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "792ab8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd169aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.5669, Valid Loss: 2.3301\n",
      "Epoch 040, Train Loss: 4.4709, Valid Loss: 2.1514\n",
      "Epoch 060, Train Loss: 4.0999, Valid Loss: 1.9826\n",
      "Epoch 080, Train Loss: 3.9182, Valid Loss: 2.0937\n",
      "Epoch 100, Train Loss: 3.8080, Valid Loss: 2.0679\n",
      "Epoch 120, Train Loss: 3.5776, Valid Loss: 2.0766\n",
      "Epoch 140, Train Loss: 3.6676, Valid Loss: 2.1193\n",
      "Epoch 160, Train Loss: 3.6619, Valid Loss: 2.0797\n",
      "Epoch 180, Train Loss: 3.7371, Valid Loss: 2.0759\n",
      "Epoch 200, Train Loss: 4.3581, Valid Loss: 2.0896\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a428f",
   "metadata": {},
   "source": [
    "* **LeakyReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29ca94cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.LeakyReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "504a1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ced2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 4.8468, Valid Loss: 2.2416\n",
      "Epoch 040, Train Loss: 4.5766, Valid Loss: 2.1587\n",
      "Epoch 060, Train Loss: 4.1499, Valid Loss: 2.0231\n",
      "Epoch 080, Train Loss: 3.8779, Valid Loss: 2.1367\n",
      "Epoch 100, Train Loss: 3.7839, Valid Loss: 2.0945\n",
      "Epoch 120, Train Loss: 3.8141, Valid Loss: 2.1212\n",
      "Epoch 140, Train Loss: 3.6841, Valid Loss: 2.1421\n",
      "Epoch 160, Train Loss: 3.6417, Valid Loss: 2.1128\n",
      "Epoch 180, Train Loss: 3.7319, Valid Loss: 2.1447\n",
      "Epoch 200, Train Loss: 4.3586, Valid Loss: 2.1514\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa416270",
   "metadata": {},
   "source": [
    "* **ELU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb01bb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ELU(alpha=1.0)\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ELU(alpha=1.0)\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ELU(alpha=1.0)\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ELU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b09e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f51fa976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 4.9892, Valid Loss: 2.3360\n",
      "Epoch 040, Train Loss: 4.5836, Valid Loss: 2.1643\n",
      "Epoch 060, Train Loss: 4.3040, Valid Loss: 2.1225\n",
      "Epoch 080, Train Loss: 3.8226, Valid Loss: 2.2610\n",
      "Epoch 100, Train Loss: 3.8776, Valid Loss: 2.2129\n",
      "Epoch 120, Train Loss: 3.8115, Valid Loss: 2.1850\n",
      "Epoch 140, Train Loss: 3.6759, Valid Loss: 2.2240\n",
      "Epoch 160, Train Loss: 3.8099, Valid Loss: 2.2118\n",
      "Epoch 180, Train Loss: 3.7035, Valid Loss: 2.2165\n",
      "Epoch 200, Train Loss: 3.8563, Valid Loss: 2.2237\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b6a07",
   "metadata": {},
   "source": [
    "* **SELU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26fc1b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): SELU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): SELU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): SELU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.SELU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a3571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31b104b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.2109, Valid Loss: 2.4745\n",
      "Epoch 040, Train Loss: 4.7176, Valid Loss: 2.2522\n",
      "Epoch 060, Train Loss: 4.2609, Valid Loss: 2.1464\n",
      "Epoch 080, Train Loss: 3.7815, Valid Loss: 2.2254\n",
      "Epoch 100, Train Loss: 3.9192, Valid Loss: 2.1982\n",
      "Epoch 120, Train Loss: 3.8465, Valid Loss: 2.2265\n",
      "Epoch 140, Train Loss: 3.6199, Valid Loss: 2.2087\n",
      "Epoch 160, Train Loss: 3.7528, Valid Loss: 2.1797\n",
      "Epoch 180, Train Loss: 3.5920, Valid Loss: 2.1847\n",
      "Epoch 200, Train Loss: 3.6484, Valid Loss: 2.1913\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bdf57",
   "metadata": {},
   "source": [
    "The best performance on the validation set seems to come from ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb22f1",
   "metadata": {},
   "source": [
    "#### Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdbb3d0",
   "metadata": {},
   "source": [
    "* **No batch normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cdb2efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08fa78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77a51bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.5669, Valid Loss: 2.3301\n",
      "Epoch 040, Train Loss: 4.4709, Valid Loss: 2.1514\n",
      "Epoch 060, Train Loss: 4.0999, Valid Loss: 1.9826\n",
      "Epoch 080, Train Loss: 3.9182, Valid Loss: 2.0937\n",
      "Epoch 100, Train Loss: 3.8080, Valid Loss: 2.0679\n",
      "Epoch 120, Train Loss: 3.5776, Valid Loss: 2.0766\n",
      "Epoch 140, Train Loss: 3.6676, Valid Loss: 2.1193\n",
      "Epoch 160, Train Loss: 3.6619, Valid Loss: 2.0797\n",
      "Epoch 180, Train Loss: 3.7371, Valid Loss: 2.0759\n",
      "Epoch 200, Train Loss: 4.3581, Valid Loss: 2.0896\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57b4f",
   "metadata": {},
   "source": [
    "* **1D batch norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5c35c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.BatchNorm1d(hidden_unit))\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65cccb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a4c5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 6.1459, Valid Loss: 2.0616\n",
      "Epoch 040, Train Loss: 5.5160, Valid Loss: 1.9682\n",
      "Epoch 060, Train Loss: 5.7625, Valid Loss: 1.9632\n",
      "Epoch 080, Train Loss: 5.0682, Valid Loss: 1.9542\n",
      "Epoch 100, Train Loss: 5.2680, Valid Loss: 2.1759\n",
      "Epoch 120, Train Loss: 4.8420, Valid Loss: 1.9381\n",
      "Epoch 140, Train Loss: 4.4300, Valid Loss: 1.9807\n",
      "Epoch 160, Train Loss: 4.0248, Valid Loss: 1.9948\n",
      "Epoch 180, Train Loss: 4.1317, Valid Loss: 1.9803\n",
      "Epoch 200, Train Loss: 4.5720, Valid Loss: 1.9469\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef50ced",
   "metadata": {},
   "source": [
    "Batch normalization makes the performance worse in this case, so I will not implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa09a6b",
   "metadata": {},
   "source": [
    "#### Learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba7d14",
   "metadata": {},
   "source": [
    "* **```learning_rate=0.001```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59f763b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "304060f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b8c3120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.5669, Valid Loss: 2.3301\n",
      "Epoch 040, Train Loss: 4.4709, Valid Loss: 2.1514\n",
      "Epoch 060, Train Loss: 4.0999, Valid Loss: 1.9826\n",
      "Epoch 080, Train Loss: 3.9182, Valid Loss: 2.0937\n",
      "Epoch 100, Train Loss: 3.8080, Valid Loss: 2.0679\n",
      "Epoch 120, Train Loss: 3.5776, Valid Loss: 2.0766\n",
      "Epoch 140, Train Loss: 3.6676, Valid Loss: 2.1193\n",
      "Epoch 160, Train Loss: 3.6619, Valid Loss: 2.0797\n",
      "Epoch 180, Train Loss: 3.7371, Valid Loss: 2.0759\n",
      "Epoch 200, Train Loss: 4.3581, Valid Loss: 2.0896\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48760601",
   "metadata": {},
   "source": [
    "* **```learning_rate=0.0001```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56e2f326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59d7d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14075617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 10.6672, Valid Loss: 2.1962\n",
      "Epoch 040, Train Loss: 4.7093, Valid Loss: 2.2749\n",
      "Epoch 060, Train Loss: 3.8926, Valid Loss: 2.2123\n",
      "Epoch 080, Train Loss: 3.5693, Valid Loss: 2.2910\n",
      "Epoch 100, Train Loss: 3.6135, Valid Loss: 2.2499\n",
      "Epoch 120, Train Loss: 3.4165, Valid Loss: 2.2592\n",
      "Epoch 140, Train Loss: 3.3112, Valid Loss: 2.2460\n",
      "Epoch 160, Train Loss: 3.4705, Valid Loss: 2.2532\n",
      "Epoch 180, Train Loss: 3.4147, Valid Loss: 2.2539\n",
      "Epoch 200, Train Loss: 3.5822, Valid Loss: 2.2623\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309ffcf",
   "metadata": {},
   "source": [
    "* **```learning_rate=0.01```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73435781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd8d2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d7d0370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 6.5789, Valid Loss: 2.1498\n",
      "Epoch 040, Train Loss: 5.4988, Valid Loss: 2.1210\n",
      "Epoch 060, Train Loss: 4.9793, Valid Loss: 1.8967\n",
      "Epoch 080, Train Loss: 4.0346, Valid Loss: 2.0415\n",
      "Epoch 100, Train Loss: 5.3023, Valid Loss: 2.0121\n",
      "Epoch 120, Train Loss: 4.1119, Valid Loss: 2.0480\n",
      "Epoch 140, Train Loss: 3.8194, Valid Loss: 2.0290\n",
      "Epoch 160, Train Loss: 4.2391, Valid Loss: 2.0796\n",
      "Epoch 180, Train Loss: 3.9281, Valid Loss: 2.0583\n",
      "Epoch 200, Train Loss: 3.8436, Valid Loss: 2.0673\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e97173",
   "metadata": {},
   "source": [
    "Decreasing the learning rate to ```0.0001``` worsens the performance on the training set. Increasing the learning rate to ```0.01``` reaches a lower performance faster, but makes the performance over epochs much more erratic. There is no  significant overall improvement in comparision to the default ```0.001```, so I will not change it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363cc661",
   "metadata": {},
   "source": [
    "#### Optimization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c8d4e",
   "metadata": {},
   "source": [
    "* **Adam optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4023c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5d802ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94152b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.5669, Valid Loss: 2.3301\n",
      "Epoch 040, Train Loss: 4.4709, Valid Loss: 2.1514\n",
      "Epoch 060, Train Loss: 4.0999, Valid Loss: 1.9826\n",
      "Epoch 080, Train Loss: 3.9182, Valid Loss: 2.0937\n",
      "Epoch 100, Train Loss: 3.8080, Valid Loss: 2.0679\n",
      "Epoch 120, Train Loss: 3.5776, Valid Loss: 2.0766\n",
      "Epoch 140, Train Loss: 3.6676, Valid Loss: 2.1193\n",
      "Epoch 160, Train Loss: 3.6619, Valid Loss: 2.0797\n",
      "Epoch 180, Train Loss: 3.7371, Valid Loss: 2.0759\n",
      "Epoch 200, Train Loss: 4.3581, Valid Loss: 2.0896\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecff5f",
   "metadata": {},
   "source": [
    "* **SGD optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a353369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54977566",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ca0ef41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 49.5324, Valid Loss: 7.2687\n",
      "Epoch 040, Train Loss: 46.2327, Valid Loss: 6.8517\n",
      "Epoch 060, Train Loss: 42.7446, Valid Loss: 6.2365\n",
      "Epoch 080, Train Loss: 39.3341, Valid Loss: 5.8618\n",
      "Epoch 100, Train Loss: 35.3740, Valid Loss: 5.1722\n",
      "Epoch 120, Train Loss: 31.6321, Valid Loss: 4.7887\n",
      "Epoch 140, Train Loss: 28.9457, Valid Loss: 4.3628\n",
      "Epoch 160, Train Loss: 27.8172, Valid Loss: 4.2317\n",
      "Epoch 180, Train Loss: 27.4354, Valid Loss: 4.0932\n",
      "Epoch 200, Train Loss: 27.0568, Valid Loss: 4.1382\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cadd0",
   "metadata": {},
   "source": [
    "Adam optimization leads to significantly better performance than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ece3db",
   "metadata": {},
   "source": [
    "#### Implement dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e51cc389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Dropout(p=0.5, inplace=False)\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    all_layers.append(nn.Dropout(p=0.5))\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71f26ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26cc181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 12.8419, Valid Loss: 2.5981\n",
      "Epoch 040, Train Loss: 11.8132, Valid Loss: 2.6594\n",
      "Epoch 060, Train Loss: 11.1727, Valid Loss: 2.7630\n",
      "Epoch 080, Train Loss: 11.2859, Valid Loss: 2.8195\n",
      "Epoch 100, Train Loss: 10.3954, Valid Loss: 2.4595\n",
      "Epoch 120, Train Loss: 10.3108, Valid Loss: 2.7423\n",
      "Epoch 140, Train Loss: 10.2185, Valid Loss: 2.6139\n",
      "Epoch 160, Train Loss: 9.4553, Valid Loss: 2.5486\n",
      "Epoch 180, Train Loss: 9.7496, Valid Loss: 2.5442\n",
      "Epoch 200, Train Loss: 9.0713, Valid Loss: 2.4131\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e019ae",
   "metadata": {},
   "source": [
    "Dropout does not help the performance of this model, so it will not be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866093f8",
   "metadata": {},
   "source": [
    "#### Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195edf61",
   "metadata": {},
   "source": [
    "* **```batch_size=64```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a450173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1601ce8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0296eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d97bf519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 5.0191, Valid Loss: 2.2429\n",
      "Epoch 040, Train Loss: 4.6213, Valid Loss: 2.1831\n",
      "Epoch 060, Train Loss: 4.0735, Valid Loss: 2.0219\n",
      "Epoch 080, Train Loss: 3.9549, Valid Loss: 2.1234\n",
      "Epoch 100, Train Loss: 3.8354, Valid Loss: 2.0831\n",
      "Epoch 120, Train Loss: 3.7275, Valid Loss: 2.0854\n",
      "Epoch 140, Train Loss: 3.6928, Valid Loss: 2.1041\n",
      "Epoch 160, Train Loss: 3.6888, Valid Loss: 2.1020\n",
      "Epoch 180, Train Loss: 3.5296, Valid Loss: 2.0844\n",
      "Epoch 200, Train Loss: 4.4159, Valid Loss: 2.1053\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070aa8a",
   "metadata": {},
   "source": [
    "* **```batch_size=32```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5057dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b7dd8e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a16e26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "526c3b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 9.7082, Valid Loss: 4.3951\n",
      "Epoch 040, Train Loss: 8.8270, Valid Loss: 4.3386\n",
      "Epoch 060, Train Loss: 8.0471, Valid Loss: 4.0919\n",
      "Epoch 080, Train Loss: 7.6728, Valid Loss: 4.1408\n",
      "Epoch 100, Train Loss: 7.7827, Valid Loss: 4.1585\n",
      "Epoch 120, Train Loss: 7.7383, Valid Loss: 4.2119\n",
      "Epoch 140, Train Loss: 6.9498, Valid Loss: 4.1421\n",
      "Epoch 160, Train Loss: 7.2664, Valid Loss: 3.9055\n",
      "Epoch 180, Train Loss: 6.8677, Valid Loss: 4.1584\n",
      "Epoch 200, Train Loss: 7.4001, Valid Loss: 4.1780\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0054a7e",
   "metadata": {},
   "source": [
    "* **```batch_size=128```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f32d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f893a82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76303858",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d147789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 2.4800, Valid Loss: 1.0850\n",
      "Epoch 040, Train Loss: 2.1697, Valid Loss: 1.0788\n",
      "Epoch 060, Train Loss: 2.2222, Valid Loss: 1.0476\n",
      "Epoch 080, Train Loss: 2.0014, Valid Loss: 1.0544\n",
      "Epoch 100, Train Loss: 2.0705, Valid Loss: 1.0673\n",
      "Epoch 120, Train Loss: 2.0171, Valid Loss: 1.0462\n",
      "Epoch 140, Train Loss: 1.9127, Valid Loss: 1.0582\n",
      "Epoch 160, Train Loss: 2.1763, Valid Loss: 1.0727\n",
      "Epoch 180, Train Loss: 2.0089, Valid Loss: 1.0535\n",
      "Epoch 200, Train Loss: 1.8305, Valid Loss: 1.0584\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802f16b",
   "metadata": {},
   "source": [
    "* **```batch_size=256```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae94b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 256\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "653de100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98be7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7be56bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 1.6488, Valid Loss: 1.0784\n",
      "Epoch 040, Train Loss: 1.3109, Valid Loss: 1.0602\n",
      "Epoch 060, Train Loss: 1.2366, Valid Loss: 1.0655\n",
      "Epoch 080, Train Loss: 1.0730, Valid Loss: 1.0564\n",
      "Epoch 100, Train Loss: 1.1606, Valid Loss: 1.0508\n",
      "Epoch 120, Train Loss: 1.1058, Valid Loss: 1.0562\n",
      "Epoch 140, Train Loss: 1.0114, Valid Loss: 1.0461\n",
      "Epoch 160, Train Loss: 1.0960, Valid Loss: 1.0505\n",
      "Epoch 180, Train Loss: 1.0684, Valid Loss: 1.0341\n",
      "Epoch 200, Train Loss: 1.1879, Valid Loss: 1.0458\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b191c65",
   "metadata": {},
   "source": [
    "Batch sizes ```128``` and ```256``` give the best performances out of the sizes tried. The difference in performance between ```128``` and ```256``` seems minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626c70e",
   "metadata": {},
   "source": [
    "### Best model (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0423c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataLoaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1baabd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [512, 256, 64]\n",
    "input_size = X.shape[1]\n",
    "all_layers = []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db36e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd10c923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020, Train Loss: 2.4800, Valid Loss: 1.0850\n",
      "Epoch 040, Train Loss: 2.1697, Valid Loss: 1.0788\n",
      "Epoch 060, Train Loss: 2.2222, Valid Loss: 1.0476\n",
      "Epoch 080, Train Loss: 2.0014, Valid Loss: 1.0544\n",
      "Epoch 100, Train Loss: 2.0705, Valid Loss: 1.0673\n",
      "Epoch 120, Train Loss: 2.0171, Valid Loss: 1.0462\n",
      "Epoch 140, Train Loss: 1.9127, Valid Loss: 1.0582\n",
      "Epoch 160, Train Loss: 2.1763, Valid Loss: 1.0727\n",
      "Epoch 180, Train Loss: 2.0089, Valid Loss: 1.0535\n",
      "Epoch 200, Train Loss: 1.8305, Valid Loss: 1.0584\n",
      "\n",
      "Final Train MSE: 1.8305\n",
      "Final Valid MSE: 1.0584\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    \n",
    "    loss_hist_valid = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dl:\n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = torch.sqrt(loss_fn(pred, y_batch))\n",
    "            loss_hist_valid += loss.item()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch:0>3}, Train Loss: 'f'{loss_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}')\n",
    "        \n",
    "    if epoch == num_epochs:\n",
    "        print()\n",
    "        print(f'Final Train MSE: {loss_hist_train:.4f}')\n",
    "        print(f'Final Valid MSE: {loss_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d433419",
   "metadata": {},
   "source": [
    "### Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "82c27a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.9465\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = torch.sqrt(loss_fn(pred, y_batch)).item()\n",
    "    print(f'Test MSE: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585d2a3",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e84fcf",
   "metadata": {},
   "source": [
    "When constructing my model I explored a number of features including: hidden unit size, hidden layers, activation functions, normalization, optimization functions, learning rate, and batch size. \n",
    "\n",
    "For the hidden unit size of individual layers, there seems to be a trend that more units leads to better performance. Adding a hidden layer also improved performance. I found ReLU to be the best performing activation function, however, I did not perform parameter explorations with the modified ReLU functions, so there is room for further testing here. Surprisingly, normalization did not improve model performance, and neither did dropout. For learning rate, 0.01 gave the best performance and for optimization functions, Adam gave the best performance. Increasing the batch size further improved the performance of my model.\n",
    "\n",
    "The NN model I developed here turned out similar to the model I developed in HW8c. They have similar number of layers with similar hidden sizes and both use the ReLU activation function. The biggest difference is that the model in HW8c implements normalization while this model does not. I am not entirely sure why normalization did not improve performance in this model. I think more testing would need to be done.\n",
    "\n",
    "Compared to my model in HW4, this NN is a vast improvement. The RMSE of the predictions using this model is orders of magnitude lower than the RMSE of the predictions using my HW4 model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
