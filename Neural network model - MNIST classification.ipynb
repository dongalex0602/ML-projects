{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d953874f",
   "metadata": {},
   "source": [
    "# Task 2. \n",
    "With MNIST data set, to randomly use 10000 images as the training set, 5000 images as the validation set and use the test set as provided, and explore how does the network architecture [number of hidden layers, number of dimensions per hidden layer, activation functions], Batch normalization, optimization algorithms, learning rate and batch size would affect the training accuracy and validation accuracy. Finally, for the best model that achieved in validation accuracy, you further train it with the whole training set, what would be your training accuracy, validation accuracy and test accuracy.  Please summarize what you have learned from this experiment (HW8b.ipynb). (30 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d51c06",
   "metadata": {},
   "source": [
    "### Download MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f332397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/alex/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <0F72FEF0-4DF1-3E8A-90BA-513122A1950F> /Users/alex/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <408F81C0-C6C8-33EF-B02E-B683F7178A71> /Users/alex/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9bac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb4bfd2c870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6378564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the MNIST dataset\n",
    "dataset = datasets.MNIST(\n",
    "    root='data/', \n",
    "    train=True, \n",
    "    transform=ToTensor(), \n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81aa5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into initial training, training, and validation datasets\n",
    "init_train, train_ds, valid_ds  = random_split(dataset, [10000, 45000, 5000])\n",
    "\n",
    "# get MNIST test dataset\n",
    "test_ds = datasets.MNIST(root='data/', train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801d1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "init_train_dl = DataLoader(init_train, batch_size, shuffle=True)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a1c45",
   "metadata": {},
   "source": [
    "### Explore models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba31c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56850a11",
   "metadata": {},
   "source": [
    "**Base model: ```model_0```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9751dd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contruct a base model\n",
    "hidden_units = [32, 16]\n",
    "image_size = init_train[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "all_layers = [nn.Flatten()]\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "    \n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model_0 = nn.Sequential(*all_layers)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12595f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_0.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e390d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_dl, valid_dl, optimizer, loss_fn, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        acc_hist_train = 0\n",
    "        loss_hist_train = 0\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            acc_hist_train += is_correct.sum()\n",
    "            loss_hist_train += loss.item() * x_batch.size(0)  # accumulate loss\n",
    "        acc_hist_train /= len(train_dl.dataset)\n",
    "        loss_hist_train /= len(train_dl.dataset)\n",
    "\n",
    "        # Validation loop\n",
    "        acc_hist_valid = 0\n",
    "        loss_hist_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "                acc_hist_valid += is_correct.sum()\n",
    "                loss_hist_valid += loss.item() * x_batch.size(0)\n",
    "            acc_hist_valid /= len(valid_dl.dataset)\n",
    "            loss_hist_valid /= len(valid_dl.dataset)\n",
    "\n",
    "        print(f'Epoch [{epoch+1:0>2}/{num_epochs}], Train Loss: {loss_hist_train:.4f}, Train Acc: {acc_hist_train:.4f}, Valid Loss: {loss_hist_valid:.4f}, Valid Acc: {acc_hist_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b36501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.3329, Train Acc: 0.5996, Valid Loss: 0.6420, Valid Acc: 0.8184\n",
      "Epoch [02/20], Train Loss: 0.5098, Train Acc: 0.8545, Valid Loss: 0.4246, Valid Acc: 0.8806\n",
      "Epoch [03/20], Train Loss: 0.3747, Train Acc: 0.8952, Valid Loss: 0.3600, Valid Acc: 0.8954\n",
      "Epoch [04/20], Train Loss: 0.3205, Train Acc: 0.9092, Valid Loss: 0.3280, Valid Acc: 0.9086\n",
      "Epoch [05/20], Train Loss: 0.2868, Train Acc: 0.9171, Valid Loss: 0.3005, Valid Acc: 0.9162\n",
      "Epoch [06/20], Train Loss: 0.2618, Train Acc: 0.9233, Valid Loss: 0.2968, Valid Acc: 0.9158\n",
      "Epoch [07/20], Train Loss: 0.2455, Train Acc: 0.9310, Valid Loss: 0.2836, Valid Acc: 0.9178\n",
      "Epoch [08/20], Train Loss: 0.2280, Train Acc: 0.9357, Valid Loss: 0.2759, Valid Acc: 0.9224\n",
      "Epoch [09/20], Train Loss: 0.2168, Train Acc: 0.9379, Valid Loss: 0.2726, Valid Acc: 0.9236\n",
      "Epoch [10/20], Train Loss: 0.2018, Train Acc: 0.9409, Valid Loss: 0.2605, Valid Acc: 0.9240\n",
      "Epoch [11/20], Train Loss: 0.1926, Train Acc: 0.9456, Valid Loss: 0.2578, Valid Acc: 0.9254\n",
      "Epoch [12/20], Train Loss: 0.1796, Train Acc: 0.9472, Valid Loss: 0.2527, Valid Acc: 0.9300\n",
      "Epoch [13/20], Train Loss: 0.1700, Train Acc: 0.9508, Valid Loss: 0.2564, Valid Acc: 0.9286\n",
      "Epoch [14/20], Train Loss: 0.1617, Train Acc: 0.9541, Valid Loss: 0.2499, Valid Acc: 0.9290\n",
      "Epoch [15/20], Train Loss: 0.1532, Train Acc: 0.9569, Valid Loss: 0.2585, Valid Acc: 0.9254\n",
      "Epoch [16/20], Train Loss: 0.1498, Train Acc: 0.9558, Valid Loss: 0.2442, Valid Acc: 0.9268\n",
      "Epoch [17/20], Train Loss: 0.1365, Train Acc: 0.9607, Valid Loss: 0.2513, Valid Acc: 0.9280\n",
      "Epoch [18/20], Train Loss: 0.1309, Train Acc: 0.9616, Valid Loss: 0.2474, Valid Acc: 0.9286\n",
      "Epoch [19/20], Train Loss: 0.1246, Train Acc: 0.9645, Valid Loss: 0.2509, Valid Acc: 0.9272\n",
      "Epoch [20/20], Train Loss: 0.1185, Train Acc: 0.9657, Valid Loss: 0.2476, Valid Acc: 0.9298\n"
     ]
    }
   ],
   "source": [
    "train_and_validate(model_0, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa070e",
   "metadata": {},
   "source": [
    "**First variation, ```model_1```: test a higher dimensional hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1724b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with a higher dimensional hidden layer\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_1 = Model()\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a71c3438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.2091, Train Acc: 0.6400, Valid Loss: 0.5031, Valid Acc: 0.8646\n",
      "Epoch [02/20], Train Loss: 0.4005, Train Acc: 0.8860, Valid Loss: 0.3543, Valid Acc: 0.8944\n",
      "Epoch [03/20], Train Loss: 0.3195, Train Acc: 0.9079, Valid Loss: 0.3141, Valid Acc: 0.9074\n",
      "Epoch [04/20], Train Loss: 0.2793, Train Acc: 0.9209, Valid Loss: 0.2962, Valid Acc: 0.9146\n",
      "Epoch [05/20], Train Loss: 0.2579, Train Acc: 0.9255, Valid Loss: 0.2874, Valid Acc: 0.9128\n",
      "Epoch [06/20], Train Loss: 0.2371, Train Acc: 0.9313, Valid Loss: 0.2770, Valid Acc: 0.9166\n",
      "Epoch [07/20], Train Loss: 0.2155, Train Acc: 0.9377, Valid Loss: 0.2624, Valid Acc: 0.9210\n",
      "Epoch [08/20], Train Loss: 0.2021, Train Acc: 0.9404, Valid Loss: 0.2547, Valid Acc: 0.9240\n",
      "Epoch [09/20], Train Loss: 0.1875, Train Acc: 0.9450, Valid Loss: 0.2652, Valid Acc: 0.9210\n",
      "Epoch [10/20], Train Loss: 0.1790, Train Acc: 0.9465, Valid Loss: 0.2546, Valid Acc: 0.9236\n",
      "Epoch [11/20], Train Loss: 0.1653, Train Acc: 0.9539, Valid Loss: 0.2468, Valid Acc: 0.9274\n",
      "Epoch [12/20], Train Loss: 0.1544, Train Acc: 0.9551, Valid Loss: 0.2441, Valid Acc: 0.9282\n",
      "Epoch [13/20], Train Loss: 0.1419, Train Acc: 0.9599, Valid Loss: 0.2368, Valid Acc: 0.9328\n",
      "Epoch [14/20], Train Loss: 0.1302, Train Acc: 0.9645, Valid Loss: 0.2288, Valid Acc: 0.9326\n",
      "Epoch [15/20], Train Loss: 0.1236, Train Acc: 0.9631, Valid Loss: 0.2378, Valid Acc: 0.9316\n",
      "Epoch [16/20], Train Loss: 0.1161, Train Acc: 0.9665, Valid Loss: 0.2317, Valid Acc: 0.9332\n",
      "Epoch [17/20], Train Loss: 0.1051, Train Acc: 0.9710, Valid Loss: 0.2324, Valid Acc: 0.9322\n",
      "Epoch [18/20], Train Loss: 0.0992, Train Acc: 0.9727, Valid Loss: 0.2295, Valid Acc: 0.9354\n",
      "Epoch [19/20], Train Loss: 0.0955, Train Acc: 0.9738, Valid Loss: 0.2287, Valid Acc: 0.9358\n",
      "Epoch [20/20], Train Loss: 0.0855, Train Acc: 0.9766, Valid Loss: 0.2489, Valid Acc: 0.9290\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_1, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd25579",
   "metadata": {},
   "source": [
    "Increasing the number of dimensions of the hidden layer allowed the model to better fit the initial training set, but there is actually a slight decrease in the performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba8aa3",
   "metadata": {},
   "source": [
    "**Second variation ```model_2```: add another hidden layer to the base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b72787b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with two hidden layers\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_2 = Model()\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d904ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.4398, Train Acc: 0.5359, Valid Loss: 0.6894, Valid Acc: 0.8024\n",
      "Epoch [02/20], Train Loss: 0.5639, Train Acc: 0.8360, Valid Loss: 0.4875, Valid Acc: 0.8560\n",
      "Epoch [03/20], Train Loss: 0.4382, Train Acc: 0.8746, Valid Loss: 0.4065, Valid Acc: 0.8838\n",
      "Epoch [04/20], Train Loss: 0.3691, Train Acc: 0.8930, Valid Loss: 0.3567, Valid Acc: 0.8940\n",
      "Epoch [05/20], Train Loss: 0.3142, Train Acc: 0.9098, Valid Loss: 0.3275, Valid Acc: 0.9028\n",
      "Epoch [06/20], Train Loss: 0.2793, Train Acc: 0.9183, Valid Loss: 0.3021, Valid Acc: 0.9074\n",
      "Epoch [07/20], Train Loss: 0.2534, Train Acc: 0.9279, Valid Loss: 0.2783, Valid Acc: 0.9202\n",
      "Epoch [08/20], Train Loss: 0.2350, Train Acc: 0.9300, Valid Loss: 0.2822, Valid Acc: 0.9166\n",
      "Epoch [09/20], Train Loss: 0.2078, Train Acc: 0.9400, Valid Loss: 0.2720, Valid Acc: 0.9192\n",
      "Epoch [10/20], Train Loss: 0.1946, Train Acc: 0.9412, Valid Loss: 0.2554, Valid Acc: 0.9218\n",
      "Epoch [11/20], Train Loss: 0.1791, Train Acc: 0.9477, Valid Loss: 0.2543, Valid Acc: 0.9280\n",
      "Epoch [12/20], Train Loss: 0.1635, Train Acc: 0.9534, Valid Loss: 0.2512, Valid Acc: 0.9274\n",
      "Epoch [13/20], Train Loss: 0.1557, Train Acc: 0.9564, Valid Loss: 0.2501, Valid Acc: 0.9262\n",
      "Epoch [14/20], Train Loss: 0.1406, Train Acc: 0.9580, Valid Loss: 0.2504, Valid Acc: 0.9278\n",
      "Epoch [15/20], Train Loss: 0.1297, Train Acc: 0.9618, Valid Loss: 0.2335, Valid Acc: 0.9328\n",
      "Epoch [16/20], Train Loss: 0.1214, Train Acc: 0.9646, Valid Loss: 0.2371, Valid Acc: 0.9346\n",
      "Epoch [17/20], Train Loss: 0.1136, Train Acc: 0.9675, Valid Loss: 0.2436, Valid Acc: 0.9300\n",
      "Epoch [18/20], Train Loss: 0.1004, Train Acc: 0.9726, Valid Loss: 0.2459, Valid Acc: 0.9332\n",
      "Epoch [19/20], Train Loss: 0.0920, Train Acc: 0.9746, Valid Loss: 0.2372, Valid Acc: 0.9348\n",
      "Epoch [20/20], Train Loss: 0.0877, Train Acc: 0.9750, Valid Loss: 0.2512, Valid Acc: 0.9308\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_2, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2abba",
   "metadata": {},
   "source": [
    "Adding a hidden layer seems to improve model performance by a noticable amount. Performance on the initial training dataloader is better than the performance of the base model. The validation loss is similar between the two models, but the validation accuracy is better for this current model. For the following variations, I will modify this model, ```model_2```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625122c",
   "metadata": {},
   "source": [
    "**Third variation ```model_3```: test different activation functions on ```model_2```**\n",
    "\n",
    "1. Add a ```softmax()``` activation function in the output layer (since it is a multi-class classification problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "777c8037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "    (7): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a model that applies softmax to the output layer\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_3 = Model()\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fec52e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 2.1295, Train Acc: 0.3400, Valid Loss: 1.9287, Valid Acc: 0.5714\n",
      "Epoch [02/20], Train Loss: 1.8162, Train Acc: 0.6743, Valid Loss: 1.7323, Valid Acc: 0.7606\n",
      "Epoch [03/20], Train Loss: 1.6902, Train Acc: 0.7909, Valid Loss: 1.6838, Valid Acc: 0.7908\n",
      "Epoch [04/20], Train Loss: 1.6622, Train Acc: 0.8098, Valid Loss: 1.6627, Valid Acc: 0.8136\n",
      "Epoch [05/20], Train Loss: 1.6487, Train Acc: 0.8213, Valid Loss: 1.6551, Valid Acc: 0.8156\n",
      "Epoch [06/20], Train Loss: 1.6408, Train Acc: 0.8282, Valid Loss: 1.6478, Valid Acc: 0.8214\n",
      "Epoch [07/20], Train Loss: 1.6321, Train Acc: 0.8356, Valid Loss: 1.6453, Valid Acc: 0.8224\n",
      "Epoch [08/20], Train Loss: 1.6278, Train Acc: 0.8387, Valid Loss: 1.6428, Valid Acc: 0.8236\n",
      "Epoch [09/20], Train Loss: 1.6241, Train Acc: 0.8428, Valid Loss: 1.6426, Valid Acc: 0.8242\n",
      "Epoch [10/20], Train Loss: 1.6188, Train Acc: 0.8474, Valid Loss: 1.6412, Valid Acc: 0.8254\n",
      "Epoch [11/20], Train Loss: 1.6166, Train Acc: 0.8485, Valid Loss: 1.6363, Valid Acc: 0.8288\n",
      "Epoch [12/20], Train Loss: 1.6138, Train Acc: 0.8523, Valid Loss: 1.6359, Valid Acc: 0.8270\n",
      "Epoch [13/20], Train Loss: 1.6123, Train Acc: 0.8521, Valid Loss: 1.6363, Valid Acc: 0.8266\n",
      "Epoch [14/20], Train Loss: 1.6087, Train Acc: 0.8564, Valid Loss: 1.6312, Valid Acc: 0.8330\n",
      "Epoch [15/20], Train Loss: 1.6053, Train Acc: 0.8600, Valid Loss: 1.6307, Valid Acc: 0.8342\n",
      "Epoch [16/20], Train Loss: 1.6029, Train Acc: 0.8613, Valid Loss: 1.6336, Valid Acc: 0.8278\n",
      "Epoch [17/20], Train Loss: 1.6030, Train Acc: 0.8615, Valid Loss: 1.6301, Valid Acc: 0.8344\n",
      "Epoch [18/20], Train Loss: 1.6023, Train Acc: 0.8612, Valid Loss: 1.6296, Valid Acc: 0.8330\n",
      "Epoch [19/20], Train Loss: 1.5992, Train Acc: 0.8647, Valid Loss: 1.6291, Valid Acc: 0.8340\n",
      "Epoch [20/20], Train Loss: 1.5976, Train Acc: 0.8669, Valid Loss: 1.6276, Valid Acc: 0.8334\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_3, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87dfa5b",
   "metadata": {},
   "source": [
    "Adding a softmax activation makes the model worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0ec42",
   "metadata": {},
   "source": [
    "2. Try a ```Tanh()``` activation function between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a8805f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with Tanh() activation function\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_3 = Model()\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9f75b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.4537, Train Acc: 0.6403, Valid Loss: 0.8828, Valid Acc: 0.8228\n",
      "Epoch [02/20], Train Loss: 0.6591, Train Acc: 0.8569, Valid Loss: 0.5250, Valid Acc: 0.8806\n",
      "Epoch [03/20], Train Loss: 0.4208, Train Acc: 0.9027, Valid Loss: 0.3892, Valid Acc: 0.9018\n",
      "Epoch [04/20], Train Loss: 0.3131, Train Acc: 0.9248, Valid Loss: 0.3280, Valid Acc: 0.9108\n",
      "Epoch [05/20], Train Loss: 0.2537, Train Acc: 0.9373, Valid Loss: 0.2949, Valid Acc: 0.9174\n",
      "Epoch [06/20], Train Loss: 0.2117, Train Acc: 0.9458, Valid Loss: 0.2664, Valid Acc: 0.9248\n",
      "Epoch [07/20], Train Loss: 0.1804, Train Acc: 0.9530, Valid Loss: 0.2456, Valid Acc: 0.9300\n",
      "Epoch [08/20], Train Loss: 0.1518, Train Acc: 0.9631, Valid Loss: 0.2413, Valid Acc: 0.9312\n",
      "Epoch [09/20], Train Loss: 0.1323, Train Acc: 0.9675, Valid Loss: 0.2369, Valid Acc: 0.9336\n",
      "Epoch [10/20], Train Loss: 0.1150, Train Acc: 0.9735, Valid Loss: 0.2274, Valid Acc: 0.9382\n",
      "Epoch [11/20], Train Loss: 0.1015, Train Acc: 0.9765, Valid Loss: 0.2366, Valid Acc: 0.9342\n",
      "Epoch [12/20], Train Loss: 0.0883, Train Acc: 0.9800, Valid Loss: 0.2278, Valid Acc: 0.9358\n",
      "Epoch [13/20], Train Loss: 0.0813, Train Acc: 0.9807, Valid Loss: 0.2286, Valid Acc: 0.9372\n",
      "Epoch [14/20], Train Loss: 0.0690, Train Acc: 0.9841, Valid Loss: 0.2300, Valid Acc: 0.9388\n",
      "Epoch [15/20], Train Loss: 0.0607, Train Acc: 0.9863, Valid Loss: 0.2372, Valid Acc: 0.9358\n",
      "Epoch [16/20], Train Loss: 0.0524, Train Acc: 0.9889, Valid Loss: 0.2349, Valid Acc: 0.9364\n",
      "Epoch [17/20], Train Loss: 0.0465, Train Acc: 0.9899, Valid Loss: 0.2418, Valid Acc: 0.9346\n",
      "Epoch [18/20], Train Loss: 0.0424, Train Acc: 0.9910, Valid Loss: 0.2319, Valid Acc: 0.9386\n",
      "Epoch [19/20], Train Loss: 0.0366, Train Acc: 0.9927, Valid Loss: 0.2357, Valid Acc: 0.9408\n",
      "Epoch [20/20], Train Loss: 0.0319, Train Acc: 0.9944, Valid Loss: 0.2328, Valid Acc: 0.9402\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_3, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac6e3d",
   "metadata": {},
   "source": [
    "The model seems to actually perform better with ```Tanh()``` than with ```ReLU()``` as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a2052",
   "metadata": {},
   "source": [
    "3. Try a ```Sigmoid()``` activation function between each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98524f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with Sigmoid activation function\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Sigmoid(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_3 = Model()\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7994310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 2.2532, Train Acc: 0.1602, Valid Loss: 2.0975, Valid Acc: 0.2210\n",
      "Epoch [02/20], Train Loss: 1.9239, Train Acc: 0.2481, Valid Loss: 1.7869, Valid Acc: 0.2842\n",
      "Epoch [03/20], Train Loss: 1.6914, Train Acc: 0.4225, Valid Loss: 1.5998, Valid Acc: 0.4738\n",
      "Epoch [04/20], Train Loss: 1.5133, Train Acc: 0.5085, Valid Loss: 1.4388, Valid Acc: 0.6022\n",
      "Epoch [05/20], Train Loss: 1.3324, Train Acc: 0.6232, Valid Loss: 1.2486, Valid Acc: 0.6350\n",
      "Epoch [06/20], Train Loss: 1.1455, Train Acc: 0.6641, Valid Loss: 1.0868, Valid Acc: 0.6626\n",
      "Epoch [07/20], Train Loss: 0.9994, Train Acc: 0.7009, Valid Loss: 0.9657, Valid Acc: 0.7132\n",
      "Epoch [08/20], Train Loss: 0.8853, Train Acc: 0.7418, Valid Loss: 0.8644, Valid Acc: 0.7494\n",
      "Epoch [09/20], Train Loss: 0.7915, Train Acc: 0.7872, Valid Loss: 0.7828, Valid Acc: 0.7858\n",
      "Epoch [10/20], Train Loss: 0.7103, Train Acc: 0.8158, Valid Loss: 0.7174, Valid Acc: 0.8208\n",
      "Epoch [11/20], Train Loss: 0.6402, Train Acc: 0.8511, Valid Loss: 0.6579, Valid Acc: 0.8414\n",
      "Epoch [12/20], Train Loss: 0.5781, Train Acc: 0.8734, Valid Loss: 0.5988, Valid Acc: 0.8634\n",
      "Epoch [13/20], Train Loss: 0.5194, Train Acc: 0.8934, Valid Loss: 0.5542, Valid Acc: 0.8762\n",
      "Epoch [14/20], Train Loss: 0.4671, Train Acc: 0.9066, Valid Loss: 0.5095, Valid Acc: 0.8850\n",
      "Epoch [15/20], Train Loss: 0.4200, Train Acc: 0.9178, Valid Loss: 0.4724, Valid Acc: 0.8922\n",
      "Epoch [16/20], Train Loss: 0.3819, Train Acc: 0.9241, Valid Loss: 0.4385, Valid Acc: 0.8972\n",
      "Epoch [17/20], Train Loss: 0.3464, Train Acc: 0.9319, Valid Loss: 0.4132, Valid Acc: 0.9040\n",
      "Epoch [18/20], Train Loss: 0.3159, Train Acc: 0.9364, Valid Loss: 0.3933, Valid Acc: 0.9052\n",
      "Epoch [19/20], Train Loss: 0.2891, Train Acc: 0.9424, Valid Loss: 0.3770, Valid Acc: 0.9074\n",
      "Epoch [20/20], Train Loss: 0.2672, Train Acc: 0.9452, Valid Loss: 0.3620, Valid Acc: 0.9124\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_3, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162c8b4",
   "metadata": {},
   "source": [
    "The best performance is still when ```Tanh()``` is used as the activation function. From now on I will use ```Tanh()``` as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5ecdc",
   "metadata": {},
   "source": [
    "**Fourth variation ```model_4```: add batch normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c29824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Tanh()\n",
       "    (9): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with batch normalization\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "model_4 = Model()\n",
    "model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d061f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.1783, Train Acc: 0.8032, Valid Loss: 0.7759, Valid Acc: 0.8764\n",
      "Epoch [02/20], Train Loss: 0.5888, Train Acc: 0.8967, Valid Loss: 0.4751, Valid Acc: 0.9046\n",
      "Epoch [03/20], Train Loss: 0.3859, Train Acc: 0.9187, Valid Loss: 0.3832, Valid Acc: 0.9064\n",
      "Epoch [04/20], Train Loss: 0.2915, Train Acc: 0.9345, Valid Loss: 0.3160, Valid Acc: 0.9166\n",
      "Epoch [05/20], Train Loss: 0.2378, Train Acc: 0.9413, Valid Loss: 0.3198, Valid Acc: 0.9092\n",
      "Epoch [06/20], Train Loss: 0.1972, Train Acc: 0.9500, Valid Loss: 0.2706, Valid Acc: 0.9240\n",
      "Epoch [07/20], Train Loss: 0.1728, Train Acc: 0.9559, Valid Loss: 0.2656, Valid Acc: 0.9222\n",
      "Epoch [08/20], Train Loss: 0.1459, Train Acc: 0.9630, Valid Loss: 0.2588, Valid Acc: 0.9246\n",
      "Epoch [09/20], Train Loss: 0.1227, Train Acc: 0.9704, Valid Loss: 0.2422, Valid Acc: 0.9278\n",
      "Epoch [10/20], Train Loss: 0.1113, Train Acc: 0.9729, Valid Loss: 0.2587, Valid Acc: 0.9228\n",
      "Epoch [11/20], Train Loss: 0.1009, Train Acc: 0.9731, Valid Loss: 0.2459, Valid Acc: 0.9232\n",
      "Epoch [12/20], Train Loss: 0.0867, Train Acc: 0.9787, Valid Loss: 0.2423, Valid Acc: 0.9286\n",
      "Epoch [13/20], Train Loss: 0.0765, Train Acc: 0.9814, Valid Loss: 0.2374, Valid Acc: 0.9318\n",
      "Epoch [14/20], Train Loss: 0.0726, Train Acc: 0.9813, Valid Loss: 0.2405, Valid Acc: 0.9284\n",
      "Epoch [15/20], Train Loss: 0.0650, Train Acc: 0.9833, Valid Loss: 0.2813, Valid Acc: 0.9186\n",
      "Epoch [16/20], Train Loss: 0.0650, Train Acc: 0.9824, Valid Loss: 0.2414, Valid Acc: 0.9306\n",
      "Epoch [17/20], Train Loss: 0.0714, Train Acc: 0.9814, Valid Loss: 0.2424, Valid Acc: 0.9312\n",
      "Epoch [18/20], Train Loss: 0.0512, Train Acc: 0.9873, Valid Loss: 0.2429, Valid Acc: 0.9336\n",
      "Epoch [19/20], Train Loss: 0.0494, Train Acc: 0.9876, Valid Loss: 0.2450, Valid Acc: 0.9310\n",
      "Epoch [20/20], Train Loss: 0.0619, Train Acc: 0.9820, Valid Loss: 0.2471, Valid Acc: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_4.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_4, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352b982",
   "metadata": {},
   "source": [
    "Batch normalization does not seem to improve the model. I will not implement it going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575f7e9",
   "metadata": {},
   "source": [
    "**Fifth variation, ```model_5```: try different optimization functions**\n",
    "\n",
    "Try SGD optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fea18969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct model with Tanh() activation function\n",
    "# this is the model with the best validation accuracy so far\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_5 = Model()\n",
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c1d44f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 2.3183, Train Acc: 0.1255, Valid Loss: 2.3075, Valid Acc: 0.1450\n",
      "Epoch [02/20], Train Loss: 2.3097, Train Acc: 0.1453, Valid Loss: 2.2996, Valid Acc: 0.1636\n",
      "Epoch [03/20], Train Loss: 2.3014, Train Acc: 0.1634, Valid Loss: 2.2920, Valid Acc: 0.1870\n",
      "Epoch [04/20], Train Loss: 2.2934, Train Acc: 0.1868, Valid Loss: 2.2845, Valid Acc: 0.2188\n",
      "Epoch [05/20], Train Loss: 2.2856, Train Acc: 0.2329, Valid Loss: 2.2772, Valid Acc: 0.2714\n",
      "Epoch [06/20], Train Loss: 2.2778, Train Acc: 0.2836, Valid Loss: 2.2698, Valid Acc: 0.3126\n",
      "Epoch [07/20], Train Loss: 2.2700, Train Acc: 0.3150, Valid Loss: 2.2622, Valid Acc: 0.3360\n",
      "Epoch [08/20], Train Loss: 2.2620, Train Acc: 0.3293, Valid Loss: 2.2545, Valid Acc: 0.3464\n",
      "Epoch [09/20], Train Loss: 2.2538, Train Acc: 0.3401, Valid Loss: 2.2464, Valid Acc: 0.3496\n",
      "Epoch [10/20], Train Loss: 2.2453, Train Acc: 0.3424, Valid Loss: 2.2380, Valid Acc: 0.3542\n",
      "Epoch [11/20], Train Loss: 2.2364, Train Acc: 0.3446, Valid Loss: 2.2290, Valid Acc: 0.3550\n",
      "Epoch [12/20], Train Loss: 2.2269, Train Acc: 0.3470, Valid Loss: 2.2194, Valid Acc: 0.3548\n",
      "Epoch [13/20], Train Loss: 2.2168, Train Acc: 0.3507, Valid Loss: 2.2092, Valid Acc: 0.3566\n",
      "Epoch [14/20], Train Loss: 2.2059, Train Acc: 0.3533, Valid Loss: 2.1981, Valid Acc: 0.3586\n",
      "Epoch [15/20], Train Loss: 2.1943, Train Acc: 0.3561, Valid Loss: 2.1863, Valid Acc: 0.3622\n",
      "Epoch [16/20], Train Loss: 2.1818, Train Acc: 0.3572, Valid Loss: 2.1735, Valid Acc: 0.3600\n",
      "Epoch [17/20], Train Loss: 2.1684, Train Acc: 0.3576, Valid Loss: 2.1597, Valid Acc: 0.3608\n",
      "Epoch [18/20], Train Loss: 2.1540, Train Acc: 0.3593, Valid Loss: 2.1450, Valid Acc: 0.3610\n",
      "Epoch [19/20], Train Loss: 2.1386, Train Acc: 0.3588, Valid Loss: 2.1294, Valid Acc: 0.3614\n",
      "Epoch [20/20], Train Loss: 2.1224, Train Acc: 0.3607, Valid Loss: 2.1128, Valid Acc: 0.3610\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_5, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392a405",
   "metadata": {},
   "source": [
    "SGD significantly decreases the accuracy of the model. I will keep Adam as the optimization function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5825753",
   "metadata": {},
   "source": [
    "**Sixth variation: try different learning rates on the performance of ```model_5```**\n",
    "\n",
    "1. learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38285425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_5 = Model()\n",
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e1792d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 0.6785, Train Acc: 0.8063, Valid Loss: 0.4129, Valid Acc: 0.8748\n",
      "Epoch [02/20], Train Loss: 0.3234, Train Acc: 0.9082, Valid Loss: 0.3549, Valid Acc: 0.8970\n",
      "Epoch [03/20], Train Loss: 0.2648, Train Acc: 0.9217, Valid Loss: 0.3262, Valid Acc: 0.9072\n",
      "Epoch [04/20], Train Loss: 0.2289, Train Acc: 0.9326, Valid Loss: 0.2967, Valid Acc: 0.9168\n",
      "Epoch [05/20], Train Loss: 0.1863, Train Acc: 0.9456, Valid Loss: 0.2818, Valid Acc: 0.9214\n",
      "Epoch [06/20], Train Loss: 0.1663, Train Acc: 0.9523, Valid Loss: 0.2710, Valid Acc: 0.9274\n",
      "Epoch [07/20], Train Loss: 0.1677, Train Acc: 0.9505, Valid Loss: 0.2817, Valid Acc: 0.9254\n",
      "Epoch [08/20], Train Loss: 0.1544, Train Acc: 0.9535, Valid Loss: 0.2901, Valid Acc: 0.9228\n",
      "Epoch [09/20], Train Loss: 0.1378, Train Acc: 0.9564, Valid Loss: 0.2870, Valid Acc: 0.9270\n",
      "Epoch [10/20], Train Loss: 0.1395, Train Acc: 0.9567, Valid Loss: 0.2686, Valid Acc: 0.9306\n",
      "Epoch [11/20], Train Loss: 0.1263, Train Acc: 0.9631, Valid Loss: 0.2969, Valid Acc: 0.9220\n",
      "Epoch [12/20], Train Loss: 0.1382, Train Acc: 0.9592, Valid Loss: 0.2697, Valid Acc: 0.9304\n",
      "Epoch [13/20], Train Loss: 0.1154, Train Acc: 0.9644, Valid Loss: 0.2649, Valid Acc: 0.9332\n",
      "Epoch [14/20], Train Loss: 0.1181, Train Acc: 0.9649, Valid Loss: 0.2685, Valid Acc: 0.9290\n",
      "Epoch [15/20], Train Loss: 0.1195, Train Acc: 0.9647, Valid Loss: 0.3055, Valid Acc: 0.9248\n",
      "Epoch [16/20], Train Loss: 0.1258, Train Acc: 0.9629, Valid Loss: 0.2836, Valid Acc: 0.9268\n",
      "Epoch [17/20], Train Loss: 0.1181, Train Acc: 0.9650, Valid Loss: 0.3319, Valid Acc: 0.9204\n",
      "Epoch [18/20], Train Loss: 0.1098, Train Acc: 0.9682, Valid Loss: 0.2704, Valid Acc: 0.9360\n",
      "Epoch [19/20], Train Loss: 0.1089, Train Acc: 0.9670, Valid Loss: 0.2710, Valid Acc: 0.9322\n",
      "Epoch [20/20], Train Loss: 0.1028, Train Acc: 0.9680, Valid Loss: 0.2820, Valid Acc: 0.9294\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model with learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_5, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347efb3f",
   "metadata": {},
   "source": [
    "The performance of the model is not as good with a larger learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6af03e",
   "metadata": {},
   "source": [
    "2. learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b69eada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_5 = Model()\n",
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27f1451b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 2.1945, Train Acc: 0.2992, Valid Loss: 2.0443, Valid Acc: 0.5058\n",
      "Epoch [02/20], Train Loss: 1.8883, Train Acc: 0.5725, Valid Loss: 1.7515, Valid Acc: 0.5958\n",
      "Epoch [03/20], Train Loss: 1.6409, Train Acc: 0.6408, Valid Loss: 1.5446, Valid Acc: 0.6668\n",
      "Epoch [04/20], Train Loss: 1.4562, Train Acc: 0.6928, Valid Loss: 1.3790, Valid Acc: 0.6996\n",
      "Epoch [05/20], Train Loss: 1.3041, Train Acc: 0.7260, Valid Loss: 1.2406, Valid Acc: 0.7290\n",
      "Epoch [06/20], Train Loss: 1.1766, Train Acc: 0.7490, Valid Loss: 1.1261, Valid Acc: 0.7528\n",
      "Epoch [07/20], Train Loss: 1.0703, Train Acc: 0.7655, Valid Loss: 1.0295, Valid Acc: 0.7688\n",
      "Epoch [08/20], Train Loss: 0.9813, Train Acc: 0.7788, Valid Loss: 0.9496, Valid Acc: 0.7840\n",
      "Epoch [09/20], Train Loss: 0.9067, Train Acc: 0.7912, Valid Loss: 0.8824, Valid Acc: 0.8002\n",
      "Epoch [10/20], Train Loss: 0.8426, Train Acc: 0.8105, Valid Loss: 0.8250, Valid Acc: 0.8164\n",
      "Epoch [11/20], Train Loss: 0.7873, Train Acc: 0.8263, Valid Loss: 0.7744, Valid Acc: 0.8290\n",
      "Epoch [12/20], Train Loss: 0.7392, Train Acc: 0.8429, Valid Loss: 0.7309, Valid Acc: 0.8418\n",
      "Epoch [13/20], Train Loss: 0.6963, Train Acc: 0.8562, Valid Loss: 0.6925, Valid Acc: 0.8520\n",
      "Epoch [14/20], Train Loss: 0.6577, Train Acc: 0.8695, Valid Loss: 0.6574, Valid Acc: 0.8646\n",
      "Epoch [15/20], Train Loss: 0.6228, Train Acc: 0.8789, Valid Loss: 0.6255, Valid Acc: 0.8672\n",
      "Epoch [16/20], Train Loss: 0.5912, Train Acc: 0.8839, Valid Loss: 0.5966, Valid Acc: 0.8760\n",
      "Epoch [17/20], Train Loss: 0.5620, Train Acc: 0.8889, Valid Loss: 0.5710, Valid Acc: 0.8802\n",
      "Epoch [18/20], Train Loss: 0.5351, Train Acc: 0.8949, Valid Loss: 0.5467, Valid Acc: 0.8834\n",
      "Epoch [19/20], Train Loss: 0.5102, Train Acc: 0.9004, Valid Loss: 0.5241, Valid Acc: 0.8884\n",
      "Epoch [20/20], Train Loss: 0.4870, Train Acc: 0.9036, Valid Loss: 0.5018, Valid Acc: 0.8914\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model with learning_rate = 0.0001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_5, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da94403",
   "metadata": {},
   "source": [
    "The model takes too long to converge with a smaller learning rate. I will keep the learning rate at 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fdf42",
   "metadata": {},
   "source": [
    "**Sixth variation: try different batch sizes on the performance of ```model_5```**\n",
    "\n",
    "1. batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b493590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 2\n",
    "\n",
    "init_train_dl = DataLoader(init_train, batch_size, shuffle=True)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3b62479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_5 = Model()\n",
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7677ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 0.5772, Train Acc: 0.8459, Valid Loss: 0.3363, Valid Acc: 0.9014\n",
      "Epoch [02/20], Train Loss: 0.2627, Train Acc: 0.9254, Valid Loss: 0.2803, Valid Acc: 0.9202\n",
      "Epoch [03/20], Train Loss: 0.2017, Train Acc: 0.9427, Valid Loss: 0.2980, Valid Acc: 0.9150\n",
      "Epoch [04/20], Train Loss: 0.1651, Train Acc: 0.9532, Valid Loss: 0.2500, Valid Acc: 0.9298\n",
      "Epoch [05/20], Train Loss: 0.1361, Train Acc: 0.9601, Valid Loss: 0.2504, Valid Acc: 0.9366\n",
      "Epoch [06/20], Train Loss: 0.1148, Train Acc: 0.9657, Valid Loss: 0.2456, Valid Acc: 0.9344\n",
      "Epoch [07/20], Train Loss: 0.0980, Train Acc: 0.9708, Valid Loss: 0.2607, Valid Acc: 0.9336\n",
      "Epoch [08/20], Train Loss: 0.0902, Train Acc: 0.9741, Valid Loss: 0.2544, Valid Acc: 0.9354\n",
      "Epoch [09/20], Train Loss: 0.0861, Train Acc: 0.9731, Valid Loss: 0.2574, Valid Acc: 0.9354\n",
      "Epoch [10/20], Train Loss: 0.0720, Train Acc: 0.9784, Valid Loss: 0.2395, Valid Acc: 0.9410\n",
      "Epoch [11/20], Train Loss: 0.0639, Train Acc: 0.9821, Valid Loss: 0.2611, Valid Acc: 0.9420\n",
      "Epoch [12/20], Train Loss: 0.0650, Train Acc: 0.9804, Valid Loss: 0.2510, Valid Acc: 0.9390\n",
      "Epoch [13/20], Train Loss: 0.0494, Train Acc: 0.9858, Valid Loss: 0.3056, Valid Acc: 0.9344\n",
      "Epoch [14/20], Train Loss: 0.0510, Train Acc: 0.9843, Valid Loss: 0.2825, Valid Acc: 0.9362\n",
      "Epoch [15/20], Train Loss: 0.0445, Train Acc: 0.9860, Valid Loss: 0.3098, Valid Acc: 0.9306\n",
      "Epoch [16/20], Train Loss: 0.0446, Train Acc: 0.9863, Valid Loss: 0.3074, Valid Acc: 0.9330\n",
      "Epoch [17/20], Train Loss: 0.0363, Train Acc: 0.9892, Valid Loss: 0.3016, Valid Acc: 0.9326\n",
      "Epoch [18/20], Train Loss: 0.0358, Train Acc: 0.9890, Valid Loss: 0.2774, Valid Acc: 0.9410\n",
      "Epoch [19/20], Train Loss: 0.0355, Train Acc: 0.9895, Valid Loss: 0.2947, Valid Acc: 0.9392\n",
      "Epoch [20/20], Train Loss: 0.0313, Train Acc: 0.9899, Valid Loss: 0.3064, Valid Acc: 0.9374\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_5, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35502269",
   "metadata": {},
   "source": [
    "The performance of the model is similar to ```batch_size=64```, but training takes significantly longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a0eec",
   "metadata": {},
   "source": [
    "2. batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99e524df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "init_train_dl = DataLoader(init_train, batch_size, shuffle=True)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11685d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_5 = Model()\n",
    "model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30c558c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.7552, Train Acc: 0.5482, Valid Loss: 1.2614, Valid Acc: 0.7464\n",
      "Epoch [02/20], Train Loss: 1.0009, Train Acc: 0.7993, Valid Loss: 0.7960, Valid Acc: 0.8392\n",
      "Epoch [03/20], Train Loss: 0.6670, Train Acc: 0.8635, Valid Loss: 0.5791, Valid Acc: 0.8748\n",
      "Epoch [04/20], Train Loss: 0.4941, Train Acc: 0.8953, Valid Loss: 0.4628, Valid Acc: 0.8974\n",
      "Epoch [05/20], Train Loss: 0.3957, Train Acc: 0.9129, Valid Loss: 0.3994, Valid Acc: 0.9040\n",
      "Epoch [06/20], Train Loss: 0.3300, Train Acc: 0.9243, Valid Loss: 0.3581, Valid Acc: 0.9108\n",
      "Epoch [07/20], Train Loss: 0.2867, Train Acc: 0.9331, Valid Loss: 0.3299, Valid Acc: 0.9164\n",
      "Epoch [08/20], Train Loss: 0.2484, Train Acc: 0.9403, Valid Loss: 0.3101, Valid Acc: 0.9170\n",
      "Epoch [09/20], Train Loss: 0.2199, Train Acc: 0.9476, Valid Loss: 0.2902, Valid Acc: 0.9224\n",
      "Epoch [10/20], Train Loss: 0.1964, Train Acc: 0.9525, Valid Loss: 0.2780, Valid Acc: 0.9226\n",
      "Epoch [11/20], Train Loss: 0.1728, Train Acc: 0.9588, Valid Loss: 0.2724, Valid Acc: 0.9270\n",
      "Epoch [12/20], Train Loss: 0.1568, Train Acc: 0.9631, Valid Loss: 0.2684, Valid Acc: 0.9242\n",
      "Epoch [13/20], Train Loss: 0.1390, Train Acc: 0.9677, Valid Loss: 0.2622, Valid Acc: 0.9260\n",
      "Epoch [14/20], Train Loss: 0.1319, Train Acc: 0.9689, Valid Loss: 0.2539, Valid Acc: 0.9294\n",
      "Epoch [15/20], Train Loss: 0.1153, Train Acc: 0.9749, Valid Loss: 0.2571, Valid Acc: 0.9272\n",
      "Epoch [16/20], Train Loss: 0.1044, Train Acc: 0.9775, Valid Loss: 0.2509, Valid Acc: 0.9302\n",
      "Epoch [17/20], Train Loss: 0.0960, Train Acc: 0.9797, Valid Loss: 0.2460, Valid Acc: 0.9316\n",
      "Epoch [18/20], Train Loss: 0.0909, Train Acc: 0.9799, Valid Loss: 0.2446, Valid Acc: 0.9336\n",
      "Epoch [19/20], Train Loss: 0.0811, Train Acc: 0.9824, Valid Loss: 0.2415, Valid Acc: 0.9344\n",
      "Epoch [20/20], Train Loss: 0.0732, Train Acc: 0.9850, Valid Loss: 0.2495, Valid Acc: 0.9308\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(model_5, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934d853",
   "metadata": {},
   "source": [
    "The performance is similar. The validation accuracy is slightly worse than when ```batch_size=64```. I will keep ```batch_size=64```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369eabb",
   "metadata": {},
   "source": [
    "### The best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef5ea48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "init_train_dl = DataLoader(init_train, batch_size, shuffle=True)\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27803940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the best performing model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 32), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(32, 16), \n",
    "            nn.Tanh(), \n",
    "            nn.Linear(16, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "best_model = Model()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af31a5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 1.4223, Train Acc: 0.6529, Valid Loss: 0.7984, Valid Acc: 0.8368\n",
      "Epoch [02/20], Train Loss: 0.6017, Train Acc: 0.8671, Valid Loss: 0.4747, Valid Acc: 0.8888\n",
      "Epoch [03/20], Train Loss: 0.3986, Train Acc: 0.9055, Valid Loss: 0.3701, Valid Acc: 0.9048\n",
      "Epoch [04/20], Train Loss: 0.3071, Train Acc: 0.9244, Valid Loss: 0.3147, Valid Acc: 0.9160\n",
      "Epoch [05/20], Train Loss: 0.2501, Train Acc: 0.9383, Valid Loss: 0.2854, Valid Acc: 0.9202\n",
      "Epoch [06/20], Train Loss: 0.2102, Train Acc: 0.9470, Valid Loss: 0.2575, Valid Acc: 0.9286\n",
      "Epoch [07/20], Train Loss: 0.1815, Train Acc: 0.9532, Valid Loss: 0.2488, Valid Acc: 0.9292\n",
      "Epoch [08/20], Train Loss: 0.1577, Train Acc: 0.9616, Valid Loss: 0.2348, Valid Acc: 0.9318\n",
      "Epoch [09/20], Train Loss: 0.1349, Train Acc: 0.9672, Valid Loss: 0.2388, Valid Acc: 0.9302\n",
      "Epoch [10/20], Train Loss: 0.1213, Train Acc: 0.9707, Valid Loss: 0.2355, Valid Acc: 0.9308\n",
      "Epoch [11/20], Train Loss: 0.1030, Train Acc: 0.9762, Valid Loss: 0.2354, Valid Acc: 0.9328\n",
      "Epoch [12/20], Train Loss: 0.0917, Train Acc: 0.9794, Valid Loss: 0.2283, Valid Acc: 0.9372\n",
      "Epoch [13/20], Train Loss: 0.0794, Train Acc: 0.9831, Valid Loss: 0.2271, Valid Acc: 0.9370\n",
      "Epoch [14/20], Train Loss: 0.0708, Train Acc: 0.9851, Valid Loss: 0.2285, Valid Acc: 0.9382\n",
      "Epoch [15/20], Train Loss: 0.0653, Train Acc: 0.9855, Valid Loss: 0.2299, Valid Acc: 0.9398\n",
      "Epoch [16/20], Train Loss: 0.0575, Train Acc: 0.9871, Valid Loss: 0.2305, Valid Acc: 0.9398\n",
      "Epoch [17/20], Train Loss: 0.0517, Train Acc: 0.9900, Valid Loss: 0.2366, Valid Acc: 0.9398\n",
      "Epoch [18/20], Train Loss: 0.0464, Train Acc: 0.9905, Valid Loss: 0.2461, Valid Acc: 0.9374\n",
      "Epoch [19/20], Train Loss: 0.0379, Train Acc: 0.9931, Valid Loss: 0.2365, Valid Acc: 0.9412\n",
      "Epoch [20/20], Train Loss: 0.0350, Train Acc: 0.9935, Valid Loss: 0.2414, Valid Acc: 0.9384\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_and_validate(best_model, init_train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d824f7",
   "metadata": {},
   "source": [
    "### Train the best model on the training dataset and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94e5f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/20], Train Loss: 0.1923, Train Acc: 0.9448, Valid Loss: 0.1694, Valid Acc: 0.9538\n",
      "Epoch [02/20], Train Loss: 0.1320, Train Acc: 0.9617, Valid Loss: 0.1595, Valid Acc: 0.9556\n",
      "Epoch [03/20], Train Loss: 0.1117, Train Acc: 0.9671, Valid Loss: 0.1513, Valid Acc: 0.9548\n",
      "Epoch [04/20], Train Loss: 0.0963, Train Acc: 0.9721, Valid Loss: 0.1451, Valid Acc: 0.9546\n",
      "Epoch [05/20], Train Loss: 0.0852, Train Acc: 0.9749, Valid Loss: 0.1435, Valid Acc: 0.9580\n",
      "Epoch [06/20], Train Loss: 0.0762, Train Acc: 0.9777, Valid Loss: 0.1447, Valid Acc: 0.9566\n",
      "Epoch [07/20], Train Loss: 0.0700, Train Acc: 0.9798, Valid Loss: 0.1364, Valid Acc: 0.9606\n",
      "Epoch [08/20], Train Loss: 0.0623, Train Acc: 0.9825, Valid Loss: 0.1487, Valid Acc: 0.9570\n",
      "Epoch [09/20], Train Loss: 0.0602, Train Acc: 0.9820, Valid Loss: 0.1444, Valid Acc: 0.9576\n",
      "Epoch [10/20], Train Loss: 0.0526, Train Acc: 0.9847, Valid Loss: 0.1369, Valid Acc: 0.9638\n",
      "Epoch [11/20], Train Loss: 0.0485, Train Acc: 0.9860, Valid Loss: 0.1436, Valid Acc: 0.9630\n",
      "Epoch [12/20], Train Loss: 0.0460, Train Acc: 0.9868, Valid Loss: 0.1465, Valid Acc: 0.9600\n",
      "Epoch [13/20], Train Loss: 0.0415, Train Acc: 0.9884, Valid Loss: 0.1466, Valid Acc: 0.9596\n",
      "Epoch [14/20], Train Loss: 0.0389, Train Acc: 0.9886, Valid Loss: 0.1443, Valid Acc: 0.9622\n",
      "Epoch [15/20], Train Loss: 0.0378, Train Acc: 0.9894, Valid Loss: 0.1454, Valid Acc: 0.9606\n",
      "Epoch [16/20], Train Loss: 0.0335, Train Acc: 0.9901, Valid Loss: 0.1511, Valid Acc: 0.9624\n",
      "Epoch [17/20], Train Loss: 0.0332, Train Acc: 0.9904, Valid Loss: 0.1506, Valid Acc: 0.9604\n",
      "Epoch [18/20], Train Loss: 0.0283, Train Acc: 0.9919, Valid Loss: 0.1538, Valid Acc: 0.9626\n",
      "Epoch [19/20], Train Loss: 0.0275, Train Acc: 0.9922, Valid Loss: 0.1688, Valid Acc: 0.9602\n",
      "Epoch [20/20], Train Loss: 0.0267, Train Acc: 0.9924, Valid Loss: 0.1548, Valid Acc: 0.9632\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate using the training set and validation set\n",
    "train_and_validate(best_model, train_dl, valid_dl, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e06a75",
   "metadata": {},
   "source": [
    "The training accuracy reaches 99.24%.\n",
    "The validation accuracy reaches 96.32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0591c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9632\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "pred = best_model(test_ds.data / 255.)\n",
    "is_correct = (torch.argmax(pred, dim=1) == test_ds.targets).float()\n",
    "print(f'Test accuracy: {is_correct.mean():.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fdeae783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 96.32%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dl:\n",
    "        outputs = best_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751fbe",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139305c4",
   "metadata": {},
   "source": [
    "There are many different ways to modify construction and training of a neural network. Sometimes the best solutions are not what is expected. I thought that a softmax() function would improve performance since this is a multi-class regression problem, however this was not the case. Additionally, I did not think that Tanh() activation fuction would give better performance than ReLU(), and again I was wrong. It's important to try many different options when developing neural networks since the best working model may not have the construction that you expect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlms2023",
   "language": "python",
   "name": "mlms2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
